{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google Colab Notbook für das LoRA-Adapter-Training\n",
        "\n",
        "Dieses Notebook wurde sowohl für das Training vom Hauptmodell als auch für das nachtraierte Modell genutzt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Der Code basiert stammt vom qlora Github-Repository, so es auch in der Arbeit zitiert ist.\n",
        "\n",
        "Allerdings wurde die qlora.py Datei aus dem Repository modifizert und findet sich im Ordner dieses Notebooks. Diese muss im geklonten Repository ersetzt werden. Für die Ausführung dieses Notebooks ist ein Hugging-Face Account notwendig, in welchem die Lizenzbedinungen von LLaMA 2 akzeptiert wurden. Der Hugging-Face Token muss in das Feld \"SECRET\" eingesetzt werden. Außerdem muss der Datensatz bei --dataset /content/qlora/data/data_diff_fixed_190124.csv platziert werden. Leider konnte der vollständige Datensatz nicht geteilt werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0V4Uv2uBs-f",
        "outputId": "7fbfd02d-420f-45af-ba48-9593c4fa6023"
      },
      "outputs": [],
      "source": [
        "!git clone \"https://github.com/artidoro/qlora/\"\n",
        "!dir\n",
        "%cd qlora\n",
        "!pip install -U -r requirements.txt\n",
        "\n",
        "!pip install huggingface_hub\n",
        "import huggingface_hub\n",
        "huggingface_hub.login(token=\"SECRET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5LCXim8yGcD",
        "outputId": "32381c20-6c12-4be6-ca54-acb7658e4ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Startet das Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xvNmLzXVNB7",
        "outputId": "4e9e56f1-5046-41b0-c03b-c5771dce7d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-2ykumo3w6vmwy --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/datalab/web/pyright/typeshed-fallback/stdlib,/usr/local/lib/python3.10/dist-packages')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 122\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda122.so...\n",
            "2024-01-25 09:57:41.031482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-25 09:57:41.031532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-25 09:57:41.032874: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-25 09:57:42.514085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Namespace(model_name_or_path='meta-llama/Llama-2-13b-hf', trust_remote_code=False, use_auth_token=True, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=1000, target_max_len=200, dataset='/content/qlora/data/data_diff_fixed_190124.csv', dataset_format='input-output', output_dir='/content/drive/MyDrive/llama-2-13b_lora', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=879, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='/content/drive/MyDrive/llama-2-13b_lora/runs/Jan25_09-57-46_a6dee3707848', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=293, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=1, past_index=-1, run_name='/content/drive/MyDrive/llama-2-13b_lora', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
            "  \"max_new_tokens\": 256,\n",
            "  \"transformers_version\": \"4.31.0\"\n",
            "}\n",
            ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=16, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
            "Found a previous checkpoint at: /content/drive/MyDrive/llama-2-13b_lora/checkpoint-586\n",
            "loading base model meta-llama/Llama-2-13b-hf...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 3/3 [02:23<00:00, 47.98s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Adding special tokens.\n",
            "Loading adapters from checkpoint.\n",
            "loaded model\n",
            "Map: 100% 4669/4669 [00:00<00:00, 13075.32 examples/s]\n",
            "trainable params: 31293440.0 || all params: 6734576640 || trainable: 0.46466825864201616\n",
            "torch.float32 390691840 0.05801282855398614\n",
            "torch.uint8 6343884800 0.9419871714460138\n",
            "  0% 0/879 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.2115, 'learning_rate': 0.0001, 'epoch': 0.03}\n",
            "{'loss': 0.3271, 'learning_rate': 0.0001, 'epoch': 0.07}\n",
            "{'loss': 0.3221, 'learning_rate': 0.0001, 'epoch': 0.1}\n",
            "{'loss': 0.3675, 'learning_rate': 0.0001, 'epoch': 0.14}\n",
            "{'loss': 0.4366, 'learning_rate': 0.0001, 'epoch': 0.17}\n",
            "{'loss': 0.343, 'learning_rate': 0.0001, 'epoch': 0.21}\n",
            "{'loss': 0.4239, 'learning_rate': 0.0001, 'epoch': 0.24}\n",
            "{'loss': 0.3796, 'learning_rate': 0.0001, 'epoch': 0.27}\n",
            "{'loss': 0.4109, 'learning_rate': 0.0001, 'epoch': 0.31}\n",
            "{'loss': 0.474, 'learning_rate': 0.0001, 'epoch': 0.34}\n",
            "{'loss': 0.3335, 'learning_rate': 0.0001, 'epoch': 0.38}\n",
            "{'loss': 0.3667, 'learning_rate': 0.0001, 'epoch': 0.41}\n",
            "{'loss': 0.3767, 'learning_rate': 0.0001, 'epoch': 0.45}\n",
            "{'loss': 0.3809, 'learning_rate': 0.0001, 'epoch': 0.48}\n",
            "{'loss': 0.4646, 'learning_rate': 0.0001, 'epoch': 0.51}\n",
            "{'loss': 0.3265, 'learning_rate': 0.0001, 'epoch': 0.55}\n",
            "{'loss': 0.389, 'learning_rate': 0.0001, 'epoch': 0.58}\n",
            "{'loss': 0.3942, 'learning_rate': 0.0001, 'epoch': 0.62}\n",
            "{'loss': 0.4303, 'learning_rate': 0.0001, 'epoch': 0.65}\n",
            "{'loss': 0.5053, 'learning_rate': 0.0001, 'epoch': 0.68}\n",
            "{'loss': 0.3415, 'learning_rate': 0.0001, 'epoch': 0.72}\n",
            "{'loss': 0.409, 'learning_rate': 0.0001, 'epoch': 0.75}\n",
            "{'loss': 0.3835, 'learning_rate': 0.0001, 'epoch': 0.79}\n",
            "{'loss': 0.4587, 'learning_rate': 0.0001, 'epoch': 0.82}\n",
            "{'loss': 0.4923, 'learning_rate': 0.0001, 'epoch': 0.86}\n",
            "{'loss': 0.3379, 'learning_rate': 0.0001, 'epoch': 0.89}\n",
            "{'loss': 0.4091, 'learning_rate': 0.0001, 'epoch': 0.92}\n",
            "{'loss': 0.4113, 'learning_rate': 0.0001, 'epoch': 0.96}\n",
            "{'loss': 0.4685, 'learning_rate': 0.0001, 'epoch': 0.99}\n",
            " 33% 293/879 [51:52<1:54:13, 11.70s/it]Saving PEFT checkpoint...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "{'loss': 0.2643, 'learning_rate': 0.0001, 'epoch': 1.03}\n",
            "{'loss': 0.2863, 'learning_rate': 0.0001, 'epoch': 1.06}\n",
            "{'loss': 0.2911, 'learning_rate': 0.0001, 'epoch': 1.1}\n",
            "{'loss': 0.2813, 'learning_rate': 0.0001, 'epoch': 1.13}\n",
            "{'loss': 0.3654, 'learning_rate': 0.0001, 'epoch': 1.16}\n",
            "{'loss': 0.2679, 'learning_rate': 0.0001, 'epoch': 1.2}\n",
            "{'loss': 0.291, 'learning_rate': 0.0001, 'epoch': 1.23}\n",
            "{'loss': 0.3038, 'learning_rate': 0.0001, 'epoch': 1.27}\n",
            "{'loss': 0.3217, 'learning_rate': 0.0001, 'epoch': 1.3}\n",
            "{'loss': 0.363, 'learning_rate': 0.0001, 'epoch': 1.34}\n",
            "{'loss': 0.2649, 'learning_rate': 0.0001, 'epoch': 1.37}\n",
            "{'loss': 0.3009, 'learning_rate': 0.0001, 'epoch': 1.4}\n",
            "{'loss': 0.3365, 'learning_rate': 0.0001, 'epoch': 1.44}\n",
            "{'loss': 0.363, 'learning_rate': 0.0001, 'epoch': 1.47}\n",
            "{'loss': 0.4193, 'learning_rate': 0.0001, 'epoch': 1.51}\n",
            "{'loss': 0.3243, 'learning_rate': 0.0001, 'epoch': 1.54}\n",
            "{'loss': 0.2981, 'learning_rate': 0.0001, 'epoch': 1.58}\n",
            "{'loss': 0.3704, 'learning_rate': 0.0001, 'epoch': 1.61}\n",
            "{'loss': 0.3586, 'learning_rate': 0.0001, 'epoch': 1.64}\n",
            "{'loss': 0.4269, 'learning_rate': 0.0001, 'epoch': 1.68}\n",
            "{'loss': 0.335, 'learning_rate': 0.0001, 'epoch': 1.71}\n",
            "{'loss': 0.3216, 'learning_rate': 0.0001, 'epoch': 1.75}\n",
            "{'loss': 0.3419, 'learning_rate': 0.0001, 'epoch': 1.78}\n",
            "{'loss': 0.3838, 'learning_rate': 0.0001, 'epoch': 1.82}\n",
            "{'loss': 0.429, 'learning_rate': 0.0001, 'epoch': 1.85}\n",
            "{'loss': 0.3282, 'learning_rate': 0.0001, 'epoch': 1.88}\n",
            "{'loss': 0.3524, 'learning_rate': 0.0001, 'epoch': 1.92}\n",
            "{'loss': 0.3757, 'learning_rate': 0.0001, 'epoch': 1.95}\n",
            "{'loss': 0.4931, 'learning_rate': 0.0001, 'epoch': 1.99}\n",
            " 67% 586/879 [1:43:45<1:06:53, 13.70s/it]Saving PEFT checkpoint...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            " 67% 589/879 [1:45:05<1:36:42, 20.01s/it]Traceback (most recent call last):\n",
            "  File \"/content/qlora/qlora.py\", line 841, in <module>\n",
            "    train()\n",
            "  File \"/content/qlora/qlora.py\", line 803, in train\n",
            "    train_result = trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2665, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 1851, in backward\n",
            "    self.scaler.scale(loss).backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            " 67% 589/879 [1:45:20<51:51, 10.73s/it]  \n"
          ]
        }
      ],
      "source": [
        "!python qlora.py --model_name_or_path meta-llama/Llama-2-13b-hf --use_auth --output_dir /content/drive/MyDrive/llama-2-13b_lora --logging_steps 10 --save_strategy steps --data_seed 42 --save_steps 293 --save_total_limit 40 --max_new_tokens 256 --dataloader_num_workers 1 --group_by_length --logging_strategy steps --remove_unused_columns False --do_train --lora_r 16 --lora_alpha 16 --lora_modules all --double_quant --quant_type nf4 --fp16 --bits 4 --warmup_ratio 0.03 --lr_scheduler_type constant --gradient_checkpointing --dataset /content/qlora/data/data_diff_fixed_190124.csv --dataset_format input-output --source_max_len 1000 --target_max_len 200 --per_device_train_batch_size 8 --gradient_accumulation_steps 2 --max_steps 879 --learning_rate 0.0001 --adam_beta2 0.999 --max_grad_norm 0.3 --lora_dropout 0.1 --weight_decay 0.0 --seed 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Speichert den Modell-Output in einer Zip-Datei"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHB9qixip2qa",
        "outputId": "0744b5eb-32ec-480a-bc75-45a2877426be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/ (stored 0%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/scheduler.pt (deflated 57%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/rng_state.pth (deflated 25%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_model/ (stored 0%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_model/README.md (deflated 39%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_model/adapter_model.bin (deflated 7%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_model/adapter_config.json (deflated 49%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/README.md (deflated 39%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/added_tokens.json (stored 0%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/training_args.bin (deflated 51%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_model.bin (deflated 7%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/tokenizer.model (deflated 55%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/adapter_config.json (deflated 49%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/trainer_state.json (deflated 84%)\n",
            "  adding: content/qlora/output/llama-2-13b_lora/checkpoint-300/tokenizer_config.json (deflated 68%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/qlora/output/llama-2-13b_lora.zip /content/qlora/output/llama-2-13b_lora/checkpoint-300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Checkpoints werden im Drive gespeichert, die Instantz wird geschlossen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "otwkiOY9D2Tc",
        "outputId": "214bba66-7bb8-4992-949f-a479e69c6b04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/LLaMA2V8'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copy(\"/content/qlora/output/llama-2-13b_lora.zip\",\"/content/drive/MyDrive/LLaMA2V8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5pfdCKDB608"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
